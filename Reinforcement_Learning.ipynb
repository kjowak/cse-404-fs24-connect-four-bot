{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Creating Connect-4 AIs using Reinforcement Learning"
   ],
   "metadata": {
    "id": "-cMoOd23fP4W"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque"
   ],
   "metadata": {
    "id": "m3dU3sPknYb7"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Connect4Env:\n",
    "    def __init__(self):\n",
    "        self.rows = 6\n",
    "        self.columns = 7\n",
    "        self.board = np.zeros((self.rows, self.columns), dtype=int)\n",
    "        self.current_player = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.rows, self.columns), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.board\n",
    "\n",
    "    def step(self, action):\n",
    "        for row in range(self.rows - 1, -1, -1):\n",
    "            if self.board[row, action] == 0:\n",
    "                self.board[row, action] = self.current_player\n",
    "                reward = 0\n",
    "                done = False\n",
    "\n",
    "                # Check win\n",
    "                if self.check_win(self.current_player):\n",
    "                    reward = 10  # Winning reward\n",
    "                    done = True\n",
    "\n",
    "                # Check draw\n",
    "                elif np.all(self.board != 0):\n",
    "                    reward = 0\n",
    "                    done = True\n",
    "\n",
    "                # Check if the move blocks the opponent\n",
    "                elif self.check_blocked_opponent():\n",
    "                    reward = 2  # Reward for blocking opponent\n",
    "\n",
    "                # Check if the move allows the opponent to win\n",
    "                elif self.allows_opponent_win():\n",
    "                    reward = -5  # Penalty for allowing opponent to win\n",
    "\n",
    "                # Switch player\n",
    "                self.current_player *= -1\n",
    "                return self.board, reward, done\n",
    "\n",
    "        raise ValueError(\"Invalid action: Column is full.\")\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        return [col for col in range(self.columns) if self.board[0, col] == 0]\n",
    "\n",
    "    def check_win(self, player_id):\n",
    "        # Horizontal\n",
    "        for row in range(self.rows):\n",
    "            for col in range(self.columns - 3):\n",
    "                if all(self.board[row, col + i] == player_id for i in range(4)):\n",
    "                    return True\n",
    "        # Vertical\n",
    "        for col in range(self.columns):\n",
    "            for row in range(self.rows - 3):\n",
    "                if all(self.board[row + i, col] == player_id for i in range(4)):\n",
    "                    return True\n",
    "        # Diagonal (top-left to bottom-right)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.columns - 3):\n",
    "                if all(self.board[row + i, col + i] == player_id for i in range(4)):\n",
    "                    return True\n",
    "        # Diagonal (top-right to bottom-left)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(3, self.columns):\n",
    "                if all(self.board[row + i, col - i] == player_id for i in range(4)):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def check_blocked_opponent(self):\n",
    "        \"\"\"Check if the current move blocks the opponent's potential 4-in-a-row.\"\"\"\n",
    "        opponent_id = -self.current_player\n",
    "        for col in range(self.columns):\n",
    "            for row in range(self.rows - 1, -1, -1):\n",
    "                if self.board[row, col] == 0:\n",
    "                    # Temporarily simulate the opponent's move\n",
    "                    self.board[row, col] = opponent_id\n",
    "                    if self.check_win(opponent_id):  # Check if opponent could win\n",
    "                        self.board[row, col] = 0  # Revert the simulated move\n",
    "                        return True\n",
    "                    self.board[row, col] = 0  # Revert the simulated move\n",
    "                    break\n",
    "        return False\n",
    "\n",
    "    def allows_opponent_win(self):\n",
    "        \"\"\"Check if the current move allows the opponent to win on their next turn.\"\"\"\n",
    "        opponent_id = -self.current_player\n",
    "        for col in range(self.columns):\n",
    "            for row in range(self.rows - 1, -1, -1):\n",
    "                if self.board[row, col] == 0:\n",
    "                    # Temporarily simulate the opponent's move\n",
    "                    self.board[row, col] = opponent_id\n",
    "                    if self.check_win(opponent_id):  # Opponent can win\n",
    "                        self.board[row, col] = 0  # Revert the simulated move\n",
    "                        return True\n",
    "                    self.board[row, col] = 0  # Revert the simulated move\n",
    "                    break\n",
    "        return False\n"
   ],
   "metadata": {
    "id": "zIB6xIZ4IUMK"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Connect4Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Connect4Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(6 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))  # First hidden layer with tanh\n",
    "        x = torch.tanh(self.fc2(x))  # Second hidden layer with tanh\n",
    "        return self.fc3(x)  # Output layer (no activation, raw Q-values)"
   ],
   "metadata": {
    "id": "o4AOlBt2n79y"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, lr=0.0001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.model = Connect4Model()\n",
    "        self.target_model = Connect4Model()\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Initialize target model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Choose an action using an epsilon-greedy policy, ensuring the action is valid.\"\"\"\n",
    "        valid_actions = env.get_valid_actions()  # Get valid columns\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)  # Random valid action\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            q_values = self.model(state)\n",
    "            # Select the best action among valid actions\n",
    "            best_action = max(valid_actions, key=lambda a: q_values[0, a].item())\n",
    "            return best_action\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute current Q-values\n",
    "        q_values = self.model(states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = self.target_model(next_states).max(1)[0]\n",
    "        targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute loss and optimize\n",
    "        loss = F.mse_loss(q_values, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n"
   ],
   "metadata": {
    "id": "_CcjG3elJoHo"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training the RL agent via self-play\n",
    "env = Connect4Env()\n",
    "agent = DQNAgent()\n",
    "episodes = 10000\n",
    "batch_size = 64\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset().flatten()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = next_state.flatten()\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Train the agent\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    agent.decay_epsilon()\n",
    "    agent.update_target_model()\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n"
   ],
   "metadata": {
    "id": "BY2GnxWmPH1A",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "daffa76b-41f7-499d-9b4c-1530e267e059"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 100/10000, Total Reward: 10, Epsilon: 0.61\n",
      "Episode 200/10000, Total Reward: 10, Epsilon: 0.37\n",
      "Episode 300/10000, Total Reward: 10, Epsilon: 0.22\n",
      "Episode 400/10000, Total Reward: 10, Epsilon: 0.13\n",
      "Episode 500/10000, Total Reward: 10, Epsilon: 0.08\n",
      "Episode 600/10000, Total Reward: 10, Epsilon: 0.05\n",
      "Episode 700/10000, Total Reward: 10, Epsilon: 0.03\n",
      "Episode 800/10000, Total Reward: 10, Epsilon: 0.02\n",
      "Episode 900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1200/10000, Total Reward: 0, Epsilon: 0.01\n",
      "Episode 1300/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1400/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1600/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1700/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 1900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2200/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2300/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2400/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2600/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2700/10000, Total Reward: 0, Epsilon: 0.01\n",
      "Episode 2800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 2900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3200/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3300/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3400/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3600/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3700/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 3900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4200/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4300/10000, Total Reward: 0, Epsilon: 0.01\n",
      "Episode 4400/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4600/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4700/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 4900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5200/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5300/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5400/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5600/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5700/10000, Total Reward: 0, Epsilon: 0.01\n",
      "Episode 5800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 5900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6200/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6300/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6400/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6600/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6700/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 6900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7200/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7300/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7400/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7600/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7700/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 7900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8200/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8300/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8400/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8600/10000, Total Reward: 0, Epsilon: 0.01\n",
      "Episode 8700/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 8900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9000/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9100/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9200/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9300/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9400/10000, Total Reward: 0, Epsilon: 0.01\n",
      "Episode 9500/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9600/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9700/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9800/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 9900/10000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 10000/10000, Total Reward: 10, Epsilon: 0.01\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Save the model\n",
    "torch.save(agent.model.state_dict(), 'RL.pth')"
   ],
   "metadata": {
    "id": "UKe4Fpimv6tE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Save the agent\n",
    "data = {\n",
    "    'model_state_dict': agent.model.state_dict(),\n",
    "    'target_model_state_dict': agent.target_model.state_dict(),\n",
    "    'epsilon': agent.epsilon,\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict()\n",
    "}\n",
    "torch.save(data, 'RL_model.pth')"
   ],
   "metadata": {
    "id": "5I1AQrQ_wm5P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "def load_model(model_class, filename):\n",
    "    model = model_class()  # Initialize the model\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    return model\n",
    "\n",
    "# Function to play against the bot\n",
    "def play_against_bot(model):\n",
    "    env = Connect4Env()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    print(\"You are Player 1 (1), and the bot is Player -1 (-1).\")\n",
    "    while not done:\n",
    "        print(\"\\nCurrent Board:\")\n",
    "        print(env.board)\n",
    "\n",
    "        # Human move\n",
    "        if env.current_player == 1:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            action = -1\n",
    "            while action not in valid_actions:\n",
    "                try:\n",
    "                    action = int(input(f\"Choose a column (0-{env.columns - 1}): \"))\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Please enter a number between 0 and 6.\")\n",
    "\n",
    "            _, reward, done = env.step(action)\n",
    "            if done:\n",
    "                print(\"\\nFinal Board:\")\n",
    "                print(env.board)\n",
    "                if reward == 10:\n",
    "                    print(\"You win!\")\n",
    "                elif reward == 0:\n",
    "                    print(\"It's a draw!\")\n",
    "                break\n",
    "\n",
    "        # Bot move\n",
    "        else:\n",
    "            state_flattened = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_flattened)\n",
    "                valid_actions = env.get_valid_actions()\n",
    "                action = max(valid_actions, key=lambda a: q_values[0, a].item())\n",
    "            print(f\"Bot chooses column: {action}\")\n",
    "            state, reward, done = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                print(\"\\nFinal Board:\")\n",
    "                print(env.board)\n",
    "                if reward == 10:\n",
    "                    print(\"The bot wins!\")\n",
    "                elif reward == 0:\n",
    "                    print(\"It's a draw!\")\n",
    "                break"
   ],
   "metadata": {
    "id": "Qur6pYvXZBsN"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = load_model(Connect4Model, 'RL_model.pth')  # Load the trained model\n",
    "play_against_bot(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_AReEfKZJhd",
    "outputId": "d5ca3d97-3be1-439e-fa4a-ab0d993c1353"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-d73156e8c743>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filename))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model loaded from RL_model.pth\n",
      "You are Player 1 (1), and the bot is Player -1 (-1).\n",
      "\n",
      "Current Board:\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "Choose a column (0-6): 3\n",
      "\n",
      "Current Board:\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "Bot chooses column: 5\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0 -1  0]]\n",
      "Choose a column (0-6): 3\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0 -1  0]]\n",
      "Bot chooses column: 5\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0 -1  0]\n",
      " [ 0  0  0  1  0 -1  0]]\n",
      "Choose a column (0-6): 3\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0 -1  0]\n",
      " [ 0  0  0  1  0 -1  0]]\n",
      "Bot chooses column: 4\n",
      "\n",
      "Current Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0 -1  0]\n",
      " [ 0  0  0  1 -1 -1  0]]\n",
      "Choose a column (0-6): 3\n",
      "\n",
      "Final Board:\n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0 -1  0]\n",
      " [ 0  0  0  1 -1 -1  0]]\n",
      "You win!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_kjtvDzl7Kdg"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
