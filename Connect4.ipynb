{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "paudCd6wo5_p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from collections import deque\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "BOARD_ROWS, BOARD_COLS = 6, 7  # Connect 4 board dimensions\n",
        "\n",
        "# Define experience replay buffer size\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "UPDATE_TARGET_FREQUENCY = 100  # How often to update the target network\n",
        "\n",
        "# Modified reward values\n",
        "ALIGN_THREE_REWARD = 0.5  # Reward for aligning three in a row\n",
        "BLOCK_OPPONENT_REWARD = 1.0  # Reward for blocking an opponent's winning move\n",
        "WIN_REWARD = 10.0\n",
        "LOSS_PENALTY = -10.0\n",
        "DRAW_REWARD = 0.5\n",
        "DRAW_PENALTY = -1"
      ],
      "metadata": {
        "id": "1RINzgn3o6w4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Connect4Env:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS), dtype=int)\n",
        "        self.current_player = 1  # Start with player 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.board.fill(0)\n",
        "        self.current_player = 1\n",
        "        return self.board\n",
        "\n",
        "    def step(self, action):\n",
        "        # Check if the chosen column is full\n",
        "        if self.board[0, action] != 0:\n",
        "            raise ValueError(\"Column is full! Invalid move.\")\n",
        "\n",
        "        # Find the next available row in the chosen column\n",
        "        row = np.max(np.where(self.board[:, action] == 0))\n",
        "        self.board[row, action] = self.current_player\n",
        "\n",
        "        done, reward = self.check_winner(row, action)\n",
        "\n",
        "        if not done:\n",
        "            # Switch to the other player if the game is not done\n",
        "            self.current_player = 3 - self.current_player\n",
        "\n",
        "        return self.board, reward, done\n",
        "\n",
        "    def available_actions(self):\n",
        "        # Return a list of columns that are not full\n",
        "        return [col for col in range(BOARD_COLS) if self.board[0, col] == 0]\n",
        "\n",
        "    def check_winner(self, row, col):\n",
        "        \"\"\"Check if there's a winner and return (done, reward).\"\"\"\n",
        "        # Define winning directions: horizontal, vertical, and two diagonals\n",
        "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
        "\n",
        "        def count_in_direction(r, c, dr, dc):\n",
        "            \"\"\"Count consecutive pieces from (r, c) in the direction (dr, dc).\"\"\"\n",
        "            count = 0\n",
        "            player = self.board[r, c]\n",
        "            while 0 <= r < BOARD_ROWS and 0 <= c < BOARD_COLS and self.board[r, c] == player:\n",
        "                count += 1\n",
        "                r += dr\n",
        "                c += dc\n",
        "            return count\n",
        "\n",
        "        for dr, dc in directions:\n",
        "            # Check in both positive and negative directions for each of the four directions\n",
        "            count = count_in_direction(row, col, dr, dc) + count_in_direction(row, col, -dr, -dc) - 1\n",
        "            if count >= 4:  # Found 4 in a row\n",
        "                return True, WIN_REWARD  # Game won by current player\n",
        "\n",
        "        if np.all(self.board != 0):  # If the board is full\n",
        "            return True, DRAW_PENALTY  # Draw game\n",
        "\n",
        "        return False, 0  # No winner yet, game continues\n",
        "\n",
        "    def has_opponent_winning_move(self):\n",
        "        # Simulate all possible moves for the opponent to see if any result in a win\n",
        "        for col in self.available_actions():\n",
        "            temp_board = self.board.copy()\n",
        "            row = np.max(np.where(temp_board[:, col] == 0))\n",
        "            temp_board[row, col] = 3 - self.current_player  # Opponent's piece\n",
        "            if self.check_winner(row, col)[0]:  # If opponent would win with this move\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def is_blocking_move(self, action):\n",
        "        # Check if playing in this column would prevent an opponent's win\n",
        "        temp_board = self.board.copy()\n",
        "        row = np.max(np.where(temp_board[:, action] == 0))\n",
        "        temp_board[row, action] = self.current_player  # AI's piece\n",
        "        return self.check_winner(row, action)[0]  # True if this move blocks opponent"
      ],
      "metadata": {
        "id": "wj7q6ynAo7vx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQNAgent, self).__init__()\n",
        "        # Define the network layers with correct input/output sizes\n",
        "        self.fc1 = nn.Linear(42, 128)  # Input layer matches the 42-board size\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 7)  # Output layer has 7 actions, one per column\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure the input is flattened to a 1D tensor with 42 elements\n",
        "        x = x.view(-1, 42)  # Reshape to [batch_size, 42]\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kWyIQUW3pCPy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experience Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def push(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        return [self.buffer[i] for i in indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "pRtFwcbGVIOP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    num_episodes = 1000  # Number of episodes to train on\n",
        "    learning_rate = 0.001\n",
        "    gamma = 0.99  # Discount factor\n",
        "    epsilon = 1.0  # Starting epsilon for exploration\n",
        "    epsilon_min = 0.1  # Minimum epsilon\n",
        "    epsilon_decay = 0.995  # Decay factor for epsilon\n",
        "\n",
        "    env = Connect4Env()  # Initialize your Connect 4 environment\n",
        "    agent = DQNAgent()  # Initialize your DQN agent\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "          print(episode)\n",
        "\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Convert state to tensor format expected by the neural network\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "\n",
        "            # Get Q-values from the agent\n",
        "            q_values = agent(state_tensor)\n",
        "\n",
        "            # Mask unavailable actions\n",
        "            available_actions = env.available_actions()  # List of available actions (columns not full)\n",
        "            mask = torch.tensor([1 if a in available_actions else 0 for a in range(7)], dtype=torch.float32)\n",
        "            masked_q_values = torch.where(mask == 1, q_values, torch.tensor(float('-inf')))\n",
        "\n",
        "            # Select an action\n",
        "            if random.random() < epsilon:\n",
        "                # Exploration: randomly select from available actions\n",
        "                action = random.choice(available_actions)\n",
        "            else:\n",
        "                # Exploitation: choose the best available action\n",
        "                action = torch.argmax(masked_q_values).item()\n",
        "\n",
        "            # Perform the action in the environment\n",
        "            next_state, reward, done = env.step(action)\n",
        "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
        "\n",
        "            # Calculate the target Q-value\n",
        "            if done:\n",
        "                target_q_value = reward  # If the game is done, reward is final\n",
        "            else:\n",
        "                next_q_values = agent(next_state_tensor)\n",
        "                next_masked_q_values = torch.where(mask == 1, next_q_values, torch.tensor(float('-inf')))\n",
        "                target_q_value = reward + gamma * torch.max(next_masked_q_values).item()\n",
        "\n",
        "            # Compute loss and update weights\n",
        "            predicted_q_value = q_values[0, action]  # Use [0, action] indexing\n",
        "            loss = criterion(predicted_q_value, torch.tensor(target_q_value, dtype=torch.float32))  # Ensure float dtype\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the state\n",
        "            state = next_state\n",
        "\n",
        "            # Decay epsilon after each step to reduce exploration\n",
        "            if epsilon > epsilon_min:\n",
        "                epsilon *= epsilon_decay\n",
        "\n",
        "    return agent  # Return the trained agent\n"
      ],
      "metadata": {
        "id": "OhQe4BT3pDrU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = train()\n",
        "print(\"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyAnL-pApEqy",
        "outputId": "a61e4cf0-f174-4a7b-9a15-7429393e4a9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(agent):\n",
        "    env = Connect4Env()\n",
        "    state = torch.tensor(env.reset(), dtype=torch.float32)  # Ensure initial state is a tensor\n",
        "    done = False\n",
        "\n",
        "    print(\"Welcome to Connect 4! You're Player 1, and the AI is Player 2.\\n\")\n",
        "\n",
        "    while not done:\n",
        "        # Display current board\n",
        "        print(\"Current board:\")\n",
        "        print(env.board)\n",
        "\n",
        "        # Human player (Player 1)\n",
        "        valid_move = False\n",
        "        while not valid_move:\n",
        "            try:\n",
        "                col = int(input(\"Enter the column (0-6) where you want to drop your piece: \"))\n",
        "                if col in env.available_actions():\n",
        "                    valid_move = True\n",
        "                else:\n",
        "                    print(\"Column is full or invalid. Choose another column.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter an integer between 0 and 6.\")\n",
        "\n",
        "        # Update the environment with human move\n",
        "        state, reward, done = env.step(col)\n",
        "        state = torch.tensor(state, dtype=torch.float32)  # Convert to tensor\n",
        "        if done:\n",
        "            print(\"You won!\")\n",
        "            break\n",
        "\n",
        "        # AI player (Player 2)\n",
        "        with torch.no_grad():\n",
        "            q_values = agent(state)\n",
        "            action = torch.argmax(q_values).item()\n",
        "\n",
        "        # Ensure AI chooses only from available actions\n",
        "        while action not in env.available_actions():\n",
        "            q_values[action] = float('-inf')  # Mask out invalid action\n",
        "            action = torch.argmax(q_values).item()\n",
        "\n",
        "        # Update the environment with AI move\n",
        "        state, reward, done = env.step(action)\n",
        "        state = torch.tensor(state, dtype=torch.float32)  # Convert to tensor\n",
        "        print(f\"AI placed its piece in column {action}.\")\n",
        "\n",
        "        # Check for game end\n",
        "        if done:\n",
        "            if reward == WIN_REWARD:\n",
        "                print(\"AI won!\")\n",
        "            elif reward == DRAW_PENALTY:\n",
        "                print(\"It's a draw!\")\n",
        "            break\n",
        "\n",
        "    # Show final board\n",
        "    print(\"Final board:\")\n",
        "    print(env.board)\n"
      ],
      "metadata": {
        "id": "hBvWG7W-pGGo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the DQNAgent is trained and we have a trained agent available as 'agent'\n",
        "play_game(agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_QA26UtplAd",
        "outputId": "fe84bb70-0190-4d3e-da8c-cefbbd364016"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Connect 4! You're Player 1, and the AI is Player 2.\n",
            "\n",
            "Current board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Enter the column (0-6) where you want to drop your piece: 3\n",
            "AI placed its piece in column 6.\n",
            "Current board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 2]]\n",
            "Enter the column (0-6) where you want to drop your piece: 3\n",
            "AI placed its piece in column 1.\n",
            "Current board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 2 0 1 0 0 2]]\n",
            "Enter the column (0-6) where you want to drop your piece: 2\n",
            "AI placed its piece in column 1.\n",
            "Current board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 2 0 1 0 0 0]\n",
            " [0 2 1 1 0 0 2]]\n",
            "Enter the column (0-6) where you want to drop your piece: 1\n",
            "AI placed its piece in column 1.\n",
            "Current board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 2 0 1 0 0 0]\n",
            " [0 2 1 1 0 0 2]]\n",
            "Enter the column (0-6) where you want to drop your piece: 4\n",
            "AI placed its piece in column 1.\n",
            "Current board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 2 0 1 0 0 0]\n",
            " [0 2 1 1 1 0 2]]\n",
            "Enter the column (0-6) where you want to drop your piece: 5\n",
            "You won!\n",
            "Final board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0]\n",
            " [0 2 0 1 0 0 0]\n",
            " [0 2 1 1 1 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EaTLSu0ppmhJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}